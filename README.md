# Sistema BBS - Bulletin Board System Distribu√≠do

## üìã √çndice

- [Sobre o Projeto](#sobre-o-projeto)
- [Arquitetura do Sistema](#arquitetura-do-sistema)
- [Tecnologias Utilizadas](#tecnologias-utilizadas)
- [Componentes do Sistema](#componentes-do-sistema)
- [Persist√™ncia Local](#persist√™ncia-local)
- [Comunica√ß√£o e Padr√µes](#comunica√ß√£o-e-padr√µes)
- [Rel√≥gios L√≥gicos e Sincroniza√ß√£o](#rel√≥gios-l√≥gicos-e-sincroniza√ß√£o)
- [Replica√ß√£o e Consist√™ncia](#replica√ß√£o-e-consist√™ncia)
- [Como Executar](#como-executar)
- [Como Testar](#como-testar)
- [Estrutura de Diret√≥rios](#estrutura-de-diret√≥rios)
- [Troubleshooting](#troubleshooting)

---

## üéØ Sobre o Projeto

O **Sistema BBS (Bulletin Board System)** √© uma implementa√ß√£o moderna e distribu√≠da inspirada nos sistemas de troca de mensagens dos anos 1970-1980. O projeto foi desenvolvido como parte da disciplina de **Sistemas Distribu√≠dos** e implementa conceitos fundamentais como:

- ‚úÖ **Comunica√ß√£o ass√≠ncrona** via ZeroMQ (REQ-REP, PUB-SUB, ROUTER-DEALER, XSUB-XPUB)
- ‚úÖ **Serializa√ß√£o bin√°ria** com MessagePack em todas as mensagens
- ‚úÖ **Balanceamento de carga** round-robin autom√°tico no broker
- ‚úÖ **Rel√≥gios l√≥gicos de Lamport** para ordena√ß√£o causal
- ‚úÖ **Sincroniza√ß√£o de rel√≥gio f√≠sico** com algoritmo de Berkeley
- ‚úÖ **Replica√ß√£o ativa** entre 3 servidores com consist√™ncia eventual
- ‚úÖ **Elei√ß√£o de coordenador** autom√°tica baseada em rank
- ‚úÖ **Persist√™ncia local** em arquivos JSON
- ‚úÖ **Hist√≥rico de mensagens** com recupera√ß√£o de canais e mensagens privadas

O sistema permite que **usu√°rios troquem mensagens privadas** e **publiquem em canais p√∫blicos**, com todas as intera√ß√µes **replicadas entre servidores** e **persistidas em disco** para recupera√ß√£o posterior.

---

## üèóÔ∏è Arquitetura do Sistema

### Diagrama de Componentes

```mermaid
graph LR
    ref[Reference Server] <--REQ/REP--> server_1[Server 1]
    ref <--REQ/REP--> server_2[Server 2]
    ref <--REQ/REP--> server_3[Server 3]

    server_1 <--REQ/REP--> broker[Broker]
    server_2 <--REQ/REP--> broker
    server_3 <--REQ/REP--> broker

    server_1 --PUB--> proxy[Proxy]
    server_2 --PUB--> proxy
    server_3 --PUB--> proxy

    broker <--REQ/REP--> client[Client]
    broker <--REQ/REP--> bot_1[Bot 1]
    broker <--REQ/REP--> bot_2[Bot 2]

    proxy --SUB--> client
    proxy --SUB--> bot_1
    proxy --SUB--> bot_2
```

### Componentes Principais

| Componente | Linguagem | Fun√ß√£o | Porta(s) |
|------------|-----------|--------|----------|
| **Broker** | C | Intermedi√°rio REQ-REP (ROUTER-DEALER), faz balanceamento de carga round-robin | 5555 (frontend), 5556 (backend) |
| **Proxy** | JavaScript | Roteador PUB-SUB (XSUB-XPUB), distribui publica√ß√µes | 5557 (XSUB), 5558 (XPUB) |
| **Reference Server** | Python | Coordena√ß√£o, atribui√ß√£o de ranks, heartbeat, elei√ß√£o | 5559 |
| **Message Server** | Python | Gerencia login, canais, mensagens, sincroniza√ß√£o Berkeley, replica√ß√£o ativa | 3 r√©plicas (porta 6000 para P2P) |
| **Client** | JavaScript | Interface interativa para o usu√°rio | - |
| **Bot** | Python | Cliente autom√°tico que gera mensagens | 2 r√©plicas |

---

## üõ†Ô∏è Tecnologias Utilizadas

### Linguagens de Programa√ß√£o

O projeto foi desenvolvido em **tr√™s linguagens diferentes**, conforme requisito:

1. **C** - Broker (comunica√ß√£o de baixo n√≠vel)
2. **JavaScript (Node.js)** - Proxy e Cliente (interface e roteamento)
3. **Python** - Servidores e Bots (l√≥gica de neg√≥cio e automa√ß√£o)

### Bibliotecas e Frameworks

- **ZeroMQ** - Biblioteca de mensageria ass√≠ncrona
  - Padr√µes: REQ-REP, PUB-SUB, ROUTER-DEALER, XSUB-XPUB
- **MessagePack** - Serializa√ß√£o bin√°ria eficiente
- **Docker & Docker Compose** - Containeriza√ß√£o e orquestra√ß√£o

### Depend√™ncias Espec√≠ficas

**Python:**
```
pyzmq>=25.0.0
msgpack>=1.0.0
```

**Node.js:**
```
zeromq: ^5.2.8
msgpack-lite: ^0.1.26
```

**C:**
```
libzmq3-dev
gcc/make
```

---

## üì¶ Componentes do Sistema

### 1. Broker (C)

**Responsabilidades:**
- Intermedi√°rio entre clientes e servidores
- Implementa padr√£o ROUTER-DEALER
- Balanceamento de carga round-robin autom√°tico
- Roteamento transparente de requisi√ß√µes

**Portas:**
- `5555` - Frontend (clientes conectam aqui)
- `5556` - Backend (servidores conectam aqui)

### 2. Proxy (JavaScript)

**Responsabilidades:**
- Roteador de publica√ß√µes PUB-SUB
- Implementa padr√£o XSUB-XPUB
- Conecta publicadores (servidores) com assinantes (clientes/bots)
- Gerencia assinaturas de t√≥picos

**Portas:**
- `5557` - XSUB (servidores publicam aqui)
- `5558` - XPUB (clientes/bots assinam aqui)

### 3. Reference Server (Python)

**Responsabilidades:**
- **Atribui√ß√£o de Ranks:** Cada servidor recebe um rank √∫nico
- **Lista de Servidores:** Mant√©m registro de todos os servidores ativos
- **Heartbeat:** Monitora sa√∫de dos servidores (timeout: 30s)
- **Coordena√ß√£o:** Base para elei√ß√£o de coordenador

**Servi√ßos:**
- `rank` - Atribui rank a um servidor
- `list` - Retorna lista de servidores ativos
- `heartbeat` - Atualiza status de servidor

### 4. Message Server (Python)

**Responsabilidades:**
- Gerenciamento de usu√°rios (login)
- Gerenciamento de canais (cria√ß√£o, listagem)
- Mensagens privadas entre usu√°rios
- Publica√ß√µes em canais p√∫blicos
- Persist√™ncia de dados
- Sincroniza√ß√£o Berkeley (coordenador)
- Replica√ß√£o ativa de estado (servidor-a-servidor)
- Hist√≥rico de mensagens

**Servi√ßos:**
- `login` - Cadastra novo usu√°rio
- `users` - Lista usu√°rios cadastrados
- `channel` - Cria novo canal
- `channels` - Lista canais dispon√≠veis
- `get_history` - Retorna hist√≥rico de canal
- `get_private_history` - Retorna mensagens privadas
- `publish` - Publica mensagem em canal
- `message` - Envia mensagem privada

**Sincroniza√ß√£o:**
- Sincroniza a cada 10 mensagens processadas
- Implementa rel√≥gio l√≥gico de Lamport
- Comunica-se com Reference Server via heartbeat

### 5. Client (JavaScript)

**Responsabilidades:**
- Interface interativa para usu√°rio
- Conex√£o ao broker para requisi√ß√µes
- Assinatura de canais e mensagens privadas
- Exibi√ß√£o de mensagens recebidas

**Comandos Dispon√≠veis:**
```
/help                    - Mostra ajuda
/users                   - Lista usu√°rios
/channels                - Lista canais
/create <canal>          - Cria canal
/join <canal>            - Inscreve-se em canal
/leave <canal>           - Cancela inscri√ß√£o
/msg <usu√°rio> <texto>   - Mensagem privada
/pub <canal> <texto>     - Publica em canal
/quit                    - Sai do cliente
```

### 6. Bot (Python)

**Responsabilidades:**
- Cliente autom√°tico para testes
- Escolhe canal aleat√≥rio
- Envia 10 mensagens predefinidas
- Loop cont√≠nuo de publica√ß√µes

**Comportamento:**
1. Faz login com nome `bot_{ID}`
2. Cria canais padr√£o (geral, tecnologia, random)
3. Escolhe canal aleat√≥rio
4. Envia 10 mensagens (delay de 2-5s entre mensagens)
5. Aguarda 5-10s e repete

---

## üíæ Persist√™ncia Local

Todos os dados s√£o **persistidos localmente em arquivos JSON**, sem uso de bancos de dados externos.

### Estrutura de Dados

```
/data/
‚îú‚îÄ‚îÄ logins.json         # Hist√≥rico de logins
‚îú‚îÄ‚îÄ channels.json       # Canais criados
‚îú‚îÄ‚îÄ messages.json       # Todas as mensagens (p√∫blicas e privadas)
‚îú‚îÄ‚îÄ reference.json      # Estado do servidor de refer√™ncia
‚îî‚îÄ‚îÄ replication/        # Dados de replica√ß√£o entre servidores
    ‚îú‚îÄ‚îÄ server_1_*.json
    ‚îú‚îÄ‚îÄ server_2_*.json
    ‚îî‚îÄ‚îÄ server_3_*.json
```

**Observa√ß√£o:** Cada servidor mant√©m c√≥pia completa dos dados ap√≥s sincroniza√ß√£o. Arquivos em `/data/replication/` s√£o backups tempor√°rios usados durante o processo de replica√ß√£o.

### Formato dos Arquivos

**logins.json:**
```json
[
  {
    "user": "alice",
    "timestamp": 1698765432.123,
    "clock": 15
  }
]
```

**channels.json:**
```json
[
  {
    "channel": "geral",
    "timestamp": 1698765450.456,
    "clock": 23
  }
]
```

**messages.json:**
```json
[
  {
    "type": "publish",
    "user": "alice",
    "channel": "geral",
    "message": "Ol√° a todos!",
    "timestamp": 1698765500.789,
    "clock": 42
  },
  {
    "type": "message",
    "src": "alice",
    "dst": "bob",
    "message": "Oi Bob!",
    "timestamp": 1698765510.123,
    "clock": 43
  }
]
```

**reference.json:**
```json
{
  "servers": {
    "server_1": {
      "rank": 1,
      "last_heartbeat": 1698765600.0
    },
    "server_2": {
      "rank": 2,
      "last_heartbeat": 1698765605.0
    }
  },
  "next_rank": 3,
  "timestamp": 1698765610.0
}
```

### Caracter√≠sticas da Persist√™ncia

- ‚úÖ **Autom√°tica:** Dados salvos ap√≥s cada opera√ß√£o
- ‚úÖ **Recuper√°vel:** Estado carregado na inicializa√ß√£o
- ‚úÖ **Thread-safe:** Usa locks para opera√ß√µes concorrentes
- ‚úÖ **Sem depend√™ncias:** Apenas arquivos JSON padr√£o
- ‚úÖ **Port√°vel:** Funciona em qualquer sistema de arquivos

---

## üîÑ Comunica√ß√£o e Padr√µes

### Padr√µes ZeroMQ Utilizados

#### 1. REQ-REP (Request-Reply)
- **Uso:** Cliente ‚Üî Servidor (via Broker)
- **Padr√£o:** S√≠ncrono, cada REQ tem um REP
- **Exemplo:** Login, listagem de usu√°rios/canais

#### 2. ROUTER-DEALER (Broker)
- **Uso:** Balanceamento de carga
- **ROUTER:** Frontend, identifica clientes
- **DEALER:** Backend, distribui round-robin para servidores

#### 3. PUB-SUB (Publisher-Subscriber)
- **Uso:** Servidor ‚Üí Clientes/Bots (via Proxy)
- **Padr√£o:** Ass√≠ncrono, baseado em t√≥picos
- **T√≥picos:** Nomes de canais e usu√°rios

#### 4. XSUB-XPUB (Proxy)
- **Uso:** Roteamento de publica√ß√µes
- **XSUB:** Recebe publica√ß√µes dos servidores
- **XPUB:** Distribui para assinantes (clientes/bots)

### Serializa√ß√£o com MessagePack

Todas as mensagens s√£o serializadas usando **MessagePack**, um formato bin√°rio eficiente:

**Vantagens:**
- ‚úÖ Menor que JSON (at√© 50% de redu√ß√£o)
- ‚úÖ Mais r√°pido para serializar/deserializar
- ‚úÖ Suporte nativo para tipos bin√°rios
- ‚úÖ Compat√≠vel entre linguagens

**Formato Padr√£o de Mensagem:**

```json
{
  "service": "nome_do_servico",
  "data": {
    "campo1": "valor1",
    "timestamp": 1698765432.123,
    "clock": 42
  }
}
```

### Formatos de Resposta por Servi√ßo

#### Servi√ßos da Parte 1 (Request-Reply)

**Login, Cria√ß√£o de Canal:** Status `"sucesso"` ou `"erro"` com campo `"description"`

```json
{
  "service": "login",
  "data": {
    "status": "sucesso",
    "timestamp": 1698765432.123,
    "clock": 43
  }
}
```

```json
{
  "service": "login",
  "data": {
    "status": "erro",
    "description": "Usu√°rio j√° cadastrado",
    "timestamp": 1698765432.123,
    "clock": 43
  }
}
```

#### Servi√ßos da Parte 2 (Publisher-Subscriber)

**Publish, Message:** Status `"OK"` ou `"erro"` com campo `"description"`

```json
{
  "service": "publish",
  "data": {
    "status": "OK",
    "timestamp": 1698765432.123,
    "clock": 44
  }
}
```

```json
{
  "service": "publish",
  "data": {
    "status": "erro",
    "description": "Canal n√£o existe",
    "timestamp": 1698765432.123,
    "clock": 44
  }
}
```

> **üìå Nota de Conformidade:**  
> A diferen√ßa entre `"sucesso"` (Parte 1) e `"OK"` (Parte 2) segue literalmente as especifica√ß√µes dos arquivos `parte1.md` e `parte2.md`. O campo de erro foi padronizado como `"description"` conforme especifica√ß√£o da Parte 1.

---

## ‚è∞ Rel√≥gios L√≥gicos e Sincroniza√ß√£o

### Rel√≥gio L√≥gico de Lamport

Implementado em **todos os processos** (clientes, servidores, bots) para ordena√ß√£o causal de eventos.

**Regras:**
1. **Incremento antes de enviar:** `clock = clock + 1`
2. **Atualiza√ß√£o ao receber:** `clock = max(clock_local, clock_recebido) + 1`
3. **Inclu√≠do em toda mensagem:** Campo `clock` em `data`

**Implementa√ß√£o:**

```python
# Python
class LogicalClock:
    def __init__(self):
        self.counter = 0
    
    def increment(self):
        self.counter += 1
        return self.counter
    
    def update(self, received_time):
        self.counter = max(self.counter, received_time) + 1
        return self.counter
```

### Sincroniza√ß√£o de Rel√≥gio F√≠sico (Berkeley)

**Objetivo:** Sincronizar rel√≥gios f√≠sicos dos 3 servidores para garantir ordena√ß√£o temporal consistente

**M√©todo Escolhido:** Algoritmo de Berkeley (vers√£o adaptada)

#### Caracter√≠sticas da Implementa√ß√£o

- ‚úÖ **Coordenador autom√°tico** baseado em rank (menor rank = coordenador)
- ‚úÖ **Sincroniza√ß√£o peri√≥dica** a cada 10 mensagens processadas
- ‚úÖ **Toler√¢ncia a falhas** com reelei√ß√£o autom√°tica
- ‚úÖ **Ajustes graduais** para evitar saltos temporais bruscos

#### Como Funciona (Classe `BerkeleySynchronizer`)

1. **Coleta de Timestamps (`collect_timestamps`)**
   - Coordenador envia requisi√ß√£o `get_time` para todos os servidores conhecidos
   - Servidores respondem com `time.time()` atual
   - Timeout de 2 segundos por servidor
   - Retorna lista de tuplas `(server_id, timestamp)`

2. **C√°lculo de Offsets (`calculate_offsets`)**
   - Calcula tempo m√©dio entre todos os servidores (incluindo coordenador)
   - Para cada servidor: `offset = average_time - server_time`
   - Armazena offsets no dicion√°rio `self.offsets`

3. **Distribui√ß√£o de Ajustes (`distribute_offsets`)**
   - Coordenador envia mensagem `adjust_time` para cada servidor
   - Payload: `{'offset': <float>}` (positivo = avan√ßar, negativo = atrasar)

4. **Aplica√ß√£o de Offset (`apply_offset`)**
   - Servidor recebe offset do coordenador
   - Atualiza `self.time_offset` (acumulativo)
   - M√©todo `get_local_time()` retorna `time.time() + self.time_offset`

5. **Ciclo Completo (`run_synchronization`)**
   - Executado apenas pelo coordenador a cada 10 mensagens
   - Sequ√™ncia: coleta ‚Üí c√°lculo ‚Üí distribui√ß√£o
   - Logs detalhados para debugging

#### Elei√ß√£o de Coordenador (Protocolo Bully)

**M√©todo Escolhido:** Algoritmo Bully completo para elei√ß√£o distribu√≠da

**Caracter√≠sticas da Implementa√ß√£o:**
- ‚úÖ **Elei√ß√£o autom√°tica** ao detectar falha do coordenador
- ‚úÖ **Comunica√ß√£o REQ-REP** via porta 6001 dedicada
- ‚úÖ **Publica√ß√£o no t√≥pico `servers`** ap√≥s elei√ß√£o
- ‚úÖ **Toler√¢ncia a falhas** com reelei√ß√£o imediata
- ‚úÖ **Persist√™ncia** de logs de elei√ß√£o em `/data/replication/election_log.json`

**Como Funciona (Classe `ElectionManager`):**

1. **Detec√ß√£o de Falha**
   - Thread `_monitor_coordinator()` verifica heartbeat do coordenador
   - Timeout configur√°vel (padr√£o: 15 segundos)
   - Se coordenador n√£o responde, inicia elei√ß√£o Bully

2. **Processo de Elei√ß√£o (Algoritmo Bully)**
   ```python
   # 1. Servidor detecta falha e inicia elei√ß√£o
   election_manager.start_election()
   
   # 2. Envia REQ 'election' para servidores com rank MAIOR
   for server in higher_rank_servers:
       send_election_request(server)  # Porta 6001
   
   # 3. Se receber OK, cancela elei√ß√£o (outro assumir√°)
   if received_ok:
       return  # Aguarda an√∫ncio
   
   # 4. Se n√£o receber OK, declara-se coordenador
   self._become_coordinator()
   ```

3. **An√∫ncio de Coordenador**
   - **Via PUB-SUB (t√≥pico `servers`):**
     ```json
     {
       "service": "election",
       "data": {
         "event": "new_coordinator",
         "coordinator": "server_1",
         "rank": 1,
         "timestamp": 1698765678.123
       }
     }
     ```
   - **Via REQ-REP (porta 6001):** Envia an√∫ncio direto para cada servidor
   - Todos os servidores atualizam `self.coordinator` e `self.is_coordinator`

4. **Formato das Mensagens**

   **Requisi√ß√£o de Elei√ß√£o:**
   ```json
   {
     "service": "election",
     "data": {
       "rank": 2,
       "server": "server_2",
       "timestamp": 1698765678.123
     }
   }
   ```

   **Resposta OK:**
   ```json
   {
     "service": "election",
     "data": {
       "status": "OK",
       "rank": 3,
       "server": "server_3",
       "timestamp": 1698765678.456
     }
   }
   ```

5. **Integra√ß√£o com Componentes**
   - **Berkeley:** Coordenador eleito executa sincroniza√ß√£o de tempo
   - **Replica√ß√£o:** Coordenador gerencia sincroniza√ß√£o de dados
   - **ReferenceServer:** Mant√©m lista autoritativa de servidores e ranks

**Diagrama do Processo:**

```mermaid
sequenceDiagram
    participant S1 as Server 1 (rank=1)
    participant S2 as Server 2 (rank=2)
    participant S3 as Server 3 (rank=3)
    participant Proxy as Proxy (t√≥pico servers)
    
    Note over S1: Coordenador falha
    S2->>S3: REQ: election (rank=2)
    S3->>S2: REP: OK (rank=3)
    Note over S2: Recebeu OK, aguarda
    S3->>Proxy: PUB: [servers] new_coordinator=S3
    Proxy->>S1: SUB: new_coordinator=S3
    Proxy->>S2: SUB: new_coordinator=S3
    Note over S3: Torna-se coordenador
```

**Vantagens:**
- ‚úÖ Elei√ß√£o r√°pida (< 5 segundos)
- ‚úÖ Servidor com maior rank sempre vence
- ‚úÖ Notifica√ß√£o imediata via PUB-SUB
- ‚úÖ Fallback REQ-REP para garantir entrega

#### Integra√ß√£o com Replica√ß√£o

A cada 10 mensagens, o coordenador executa:
1. **Sincroniza√ß√£o Berkeley** (`_run_berkeley_sync()`)
2. **Replica√ß√£o de dados** (`_replicate_current_state()`)

Isso garante que:
- Timestamps nas mensagens replicadas sejam consistentes
- Hist√≥rico de mensagens tenha ordena√ß√£o temporal correta
- Logs de diferentes servidores sejam compar√°veis

---

## üîÅ Replica√ß√£o e Consist√™ncia (Parte 5 do Projeto)

### Contexto e Problema

Com balanceamento de carga **round-robin** no broker, cada servidor recebe apenas parte das requisi√ß√µes dos clientes. Sem replica√ß√£o, isso causa:

- ‚ùå **Perda de dados** se um servidor falhar
- ‚ùå **Hist√≥rico incompleto** em cada servidor (cliente v√™ apenas parte das mensagens)
- ‚ùå **Inconsist√™ncia entre r√©plicas** (cada servidor tem estado diferente)
- ‚ùå **Impossibilidade de recuperar hist√≥rico completo** de canais

**Requisito da Parte 5:** Implementar m√©todo de replica√ß√£o para que **todos os servidores tenham todos os dados**, documentando a escolha e estrat√©gia.

---

### 1. M√©todo Escolhido: **Replica√ß√£o Ativa (Active Replication)**

#### O que √© Replica√ß√£o Ativa?

Na **Replica√ß√£o Ativa**, todos os servidores:
- ‚úÖ Processam requisi√ß√µes dos clientes (via balanceamento de carga)
- ‚úÖ Mant√™m **c√≥pias completas** do estado do sistema
- ‚úÖ Sincronizam periodicamente entre si (peer-to-peer)
- ‚úÖ N√£o h√° distin√ß√£o entre "prim√°rio" e "backup"

**Contraste com Replica√ß√£o Passiva:**
- Na **Replica√ß√£o Passiva (Primary-Backup)**, apenas o servidor prim√°rio processa requisi√ß√µes e propaga mudan√ßas para backups (que ficam inativos at√© falha do prim√°rio).

---

### 2. Justificativa da Escolha

> **üìå Conformidade com Especifica√ß√£o (Parte 5):**  
> Conforme requisitado em `parte5.md` linha 17, esta se√ß√£o **"descreve o m√©todo escolhido dentre os que j√° existem"** atrav√©s de an√°lise comparativa entre Replica√ß√£o Ativa, Replica√ß√£o Passiva e algoritmos de consenso (Paxos/Raft).

#### Por que Replica√ß√£o Ativa?

**An√°lise Comparativa de M√©todos de Replica√ß√£o:**

| Crit√©rio | Replica√ß√£o Ativa<br/>(Escolhido ‚úÖ) | Replica√ß√£o Passiva<br/>(Primary-Backup) | Consenso Distribu√≠do<br/>(Paxos/Raft) |
|----------|------------------|-------------------|------------|
| **Toler√¢ncia a falhas** | ‚úÖ Alta (qualquer servidor pode falhar) | ‚ö†Ô∏è M√©dia (falha do prim√°rio requer elei√ß√£o) | ‚úÖ Alta |
| **Disponibilidade** | ‚úÖ M√°xima (todos processam) | ‚ö†Ô∏è Limitada (apenas prim√°rio processa) | ‚úÖ Alta |
| **Complexidade** | ‚ö†Ô∏è M√©dia (sincroniza√ß√£o P2P) | ‚úÖ Baixa (apenas prim√°rio‚Üíbackups) | ‚ùå Alta (consenso distribu√≠do) |
| **Performance** | ‚úÖ Boa (carga distribu√≠da) | ‚ö†Ô∏è Prim√°rio pode ser gargalo | ‚ö†Ô∏è Overhead de consenso |
| **Consist√™ncia** | ‚ö†Ô∏è Eventual | ‚úÖ Forte (prim√°rio serializa) | ‚úÖ Forte (consenso) |
| **Adequa√ß√£o ao BBS** | ‚úÖ‚úÖ‚úÖ Ideal | ‚ö†Ô∏è Subutiliza servidores | ‚ùå Overkill para chat |

**Decis√£o:** Replica√ß√£o Ativa √© ideal porque:
1. ‚úÖ **Sistema j√° tem balanceamento de carga** ‚Äî todos os servidores devem responder
2. ‚úÖ **Consist√™ncia eventual √© aceit√°vel** ‚Äî em um chat, pequenos atrasos (< 1s) s√£o toler√°veis
3. ‚úÖ **Simplicidade de implementa√ß√£o** ‚Äî sem necessidade de algoritmos complexos de consenso
4. ‚úÖ **Alta disponibilidade** ‚Äî sistema continua funcionando mesmo com 2 dos 3 servidores offline

---

### 3. Protocolo de Sincroniza√ß√£o

#### Comunica√ß√£o Servidor-a-Servidor

Cada servidor:
- **Exp√µe um socket REP na porta 6000** para receber requisi√ß√µes de outros servidores
- **Usa sockets REQ** para enviar dados aos outros servidores
- **N√£o h√° intermedi√°rios** (comunica√ß√£o direta peer-to-peer)

**Endere√ßamento:** `tcp://server_1:6000`, `tcp://server_2:6000`, `tcp://server_3:6000`

**Raz√£o para porta 6000:** Separa√ß√£o l√≥gica da porta de neg√≥cio (5556/broker), evitando conflitos e facilitando firewall/logs.

#### Tipos de Mensagens Trocadas

##### 3.1. Mensagem `replicate` ‚Äî Sincroniza√ß√£o de Dados

**Dire√ß√£o:** Servidor_A ‚Üí Servidor_B (ap√≥s processar 10 mensagens)

**Formato (serializado com MessagePack):**
```json
{
  "service": "replicate",
  "data": {
    "source_server": "server_1",
    "type": "logins|channels|messages",
    "payload": [
      {"user": "alice", "timestamp": 1698765432.123},
      {"user": "bob", "timestamp": 1698765433.456}
    ],
    "timestamp": 1698765440.789,
    "clock": 142
  }
}
```

**Resposta:**
```json
{
  "service": "replicate",
  "data": {
    "status": "success|error",
    "records_received": 25,
    "timestamp": 1698765441.012,
    "clock": 143
  }
}
```

##### 3.2. Mensagem `sync_state` ‚Äî Recupera√ß√£o Completa

**Dire√ß√£o:** Servidor_novo ‚Üí Servidor_coordenador (ao reiniciar)

**Formato:**
```json
{
  "service": "sync_state",
  "data": {
    "requester": "server_2",
    "timestamp": 1698765450.000,
    "clock": 5
  }
}
```

**Resposta:**
```json
{
  "service": "sync_state",
  "data": {
    "status": "success",
    "state": {
      "logins": [...],    // Estado completo
      "channels": [...],
      "messages": [...]
    },
    "timestamp": 1698765451.234,
    "clock": 200
  }
}
```

##### 3.3. Mensagem `get_time` ‚Äî Berkeley Sync

Usada pelo coordenador para coletar timestamps na sincroniza√ß√£o de rel√≥gio f√≠sico.

```json
{
  "service": "get_time",
  "data": {
    "requester": "server_1",
    "timestamp": 1698765460.000
  }
}
```

**Resposta:**
```json
{
  "service": "get_time",
  "data": {
    "time": 1698765460.123,
    "server": "server_2"
  }
}
```

---

### 4. Fluxo de Opera√ß√µes

#### 4.1. Sincroniza√ß√£o Peri√≥dica (Cen√°rio Normal)

```mermaid
sequenceDiagram
    participant C as Cliente
    participant B as Broker
    participant S1 as Server_1
    participant S2 as Server_2
    participant S3 as Server_3

    C->>B: REQ: login(alice)
    B->>S1: Roteamento round-robin
    S1->>S1: Processa + persiste local
    S1->>B: REP: sucesso
    B->>C: REP: sucesso
    
    Note over S1: Ap√≥s 10 mensagens...
    S1->>S1: _check_sync() disparado
    S1->>S1: _save_state()
    
    par Replica√ß√£o Paralela
        S1->>S2: REQ: replicate(logins)
        S2->>S1: REP: success
    and
        S1->>S3: REQ: replicate(logins)
        S3->>S1: REP: success
    end
    
    par Replica√ß√£o Paralela
        S1->>S2: REQ: replicate(channels)
        S2->>S1: REP: success
    and
        S1->>S3: REQ: replicate(channels)
        S3->>S1: REP: success
    end
    
    par Replica√ß√£o Paralela
        S1->>S2: REQ: replicate(messages)
        S2->>S1: REP: success
    and
        S1->>S3: REQ: replicate(messages)
        S3->>S1: REP: success
    end
    
    Note over S1,S3: Todos os servidores t√™m estado completo
```

**C√≥digo correspondente (`server.py`, linha 297):**
```python
def _check_sync(self):
    if self.message_count % SYNC_INTERVAL == 0:  # A cada 10 mensagens
        self._save_state()
        if self.replication_manager:
            Thread(target=self._replicate_current_state, daemon=True).start()
```

**Implementa√ß√£o (`replication_manager.py`, linha 237):**
```python
def replicate_to_all(self, data_type: str, payload: Any):
    for server in self.known_servers:
        self._replicate_to_server(server['name'], data_type, payload)
```

#### 4.2. Recupera√ß√£o ap√≥s Falha

**Cen√°rio:** `server_2` reinicia ap√≥s crash

```mermaid
sequenceDiagram
    participant S2 as Server_2 (reiniciando)
    participant Ref as ReferenceServer
    participant S1 as Server_1 (coordenador)

    S2->>S2: __init__() ‚Üí _load_state()
    Note over S2: Estado local pode estar desatualizado
    
    S2->>Ref: REQ: rank(server_2)
    Ref->>S2: REP: rank=2
    
    S2->>Ref: REQ: list()
    Ref->>S2: REP: [server_1, server_2, server_3]
    
    Note over S2: Identifica coordenador (menor rank)
    
    S2->>S1: REQ: sync_state()
    S1->>S1: Coleta logins, channels, messages
    S1->>S2: REP: state{logins, channels, messages}
    
    S2->>S2: Aplica estado completo
    S2->>S2: Salva localmente
    
    Note over S2: Servidor pronto e sincronizado
```

**C√≥digo (`replication_manager.py`, linha 290):**
```python
def sync_from_coordinator(self, coordinator_name: str) -> bool:
    request = msgpack.packb({'service': 'sync_state', 'data': {...}})
    socket.send(request)
    response = msgpack.unpackb(socket.recv(), raw=False)
    
    if response['data']['status'] == 'success':
        state = response['data']['state']
        self.datastore.save('logins.json', state['logins'])
        self.datastore.save('channels.json', state['channels'])
        self.datastore.save('messages.json', state['messages'])
```

#### 4.3. Quem Inicia a Replica√ß√£o?

**Estrat√©gia Push (implementada):**
- ‚úÖ Servidor que **processa** a requisi√ß√£o **envia** para os outros
- ‚úÖ A cada 10 mensagens, servidor faz `replicate_to_all()`
- ‚úÖ N√£o espera confirma√ß√£o antes de responder ao cliente (ass√≠ncrono)

**Alternativa Pull (n√£o implementada):**
- Servidores periodicamente **solicitam** dados de outros
- Maior lat√™ncia, mas menor overhead se poucos dados mudarem

---

### 5. Estrat√©gia de Ordena√ß√£o e Resolu√ß√£o de Conflitos

#### 5.1. Ordena√ß√£o Causal com Rel√≥gios L√≥gicos

Cada mensagem inclui:
- `timestamp` (float): Rel√≥gio f√≠sico do servidor (`time.time()`)
- `clock` (int): Rel√≥gio l√≥gico de Lamport

**Regra de ordena√ß√£o:**
```python
def compare(msg1, msg2):
    if msg1['clock'] != msg2['clock']:
        return msg1['clock'] < msg2['clock']  # Lamport decide
    else:
        return msg1['timestamp'] < msg2['timestamp']  # Empate: timestamp f√≠sico
```

#### 5.2. Resolu√ß√£o de Conflitos: Modelo Append-Only

**Estrat√©gia:** N√£o h√° conflitos porque:
1. **Mensagens s√£o imut√°veis** ‚Äî Uma vez criada, n√£o √© editada nem deletada
2. **Opera√ß√µes s√£o comutativas** ‚Äî Ordem de replica√ß√£o n√£o afeta resultado final
   - `add_user('alice')` + `add_user('bob')` = `add_user('bob')` + `add_user('alice')`
3. **Replica√ß√£o substitui√ß√£o completa** ‚Äî Dados replicados **sobrescrevem** completamente

**Exemplo:**
```python
# Server_1 tem: {users: ['alice', 'bob']}
# Server_2 tem: {users: ['alice']}

# Server_1 replica para Server_2:
replicate(users=['alice', 'bob', 'charlie'])

# Server_2 ap√≥s replica√ß√£o: {users: ['alice', 'bob', 'charlie']}
# Substitui√ß√£o completa, sem merge
```

**C√≥digo (`replication_manager.py`, linha 94):**
```python
def _handle_replicate(self, data: Dict) -> Dict:
    payload = data.get('payload', [])
    if data_type == 'logins':
        self.datastore.save('logins.json', payload)  # Substitui√ß√£o completa
```

#### 5.3. Garantia de Entrega

**Mecanismo:**
- ‚úÖ **Timeout de 3 segundos** (`zmq.RCVTIMEO = 3000`)
- ‚úÖ **Log de erros** se replica√ß√£o falhar
- ‚ö†Ô∏è **Sem retry autom√°tico** (eventual consistency tolerante a falhas tempor√°rias)

**C√≥digo (`replication_manager.py`, linha 262):**
```python
try:
    socket.send(request)
    response = socket.recv()  # Timeout 3s
    if response['data']['status'] == 'success':
        print(f"Dados replicados para {target_server}")
except Exception as e:
    print(f"Erro ao replicar para {target_server}: {e}")
    # Continua sem bloqueio
```

---

### 6. Limita√ß√µes Conhecidas

| Limita√ß√£o | Descri√ß√£o | Impacto | Mitiga√ß√£o |
|-----------|-----------|---------|-----------|
| **Consist√™ncia Eventual** | Janela de at√© 10 mensagens onde servidores divergem | ‚ö†Ô∏è M√©dio | Aceit√°vel para chat; usu√°rio pode ver mensagem atrasada (< 1s) |
| **Sem Resolu√ß√£o de Conflitos Concorrentes** | Se dois servidores criam canais com mesmo nome simultaneamente | ‚ö†Ô∏è Baixo | Improv√°vel; mesmo que ocorra, √∫ltima replica√ß√£o vence |
| **Perda de Dados se Todos Falharem** | Se todos os 3 servidores ca√≠rem antes de persistir | ‚ùå Alto | Dados j√° processados est√£o salvos em disco; apenas √∫ltimas < 10 mensagens perdem |
| **Sem Transa√ß√µes Distribu√≠das** | Replica√ß√£o pode falhar parcialmente (ex: replica logins mas n√£o channels) | ‚ö†Ô∏è M√©dio | Pr√≥xima sincroniza√ß√£o corrige; dados s√£o eventualmente consistentes |
| **Largura de Banda** | Replica√ß√£o completa a cada 10 mensagens pode sobrecarregar rede | ‚ö†Ô∏è Baixo | Para < 1000 msg/s √© aceit√°vel; considerar delta-sync para escala maior |
| **Lat√™ncia de Sincroniza√ß√£o** | Atraso de at√© 10 mensagens (~ 1-5 segundos) | ‚ö†Ô∏è Baixo | Trade-off consciente: performance vs consist√™ncia |

---

### 7. Persist√™ncia dos Dados Replicados

**Estrutura de Diret√≥rios:**
```
/data/
‚îú‚îÄ‚îÄ logins.json              # Estado compartilhado
‚îú‚îÄ‚îÄ channels.json            # Estado compartilhado
‚îú‚îÄ‚îÄ messages.json            # Estado compartilhado
‚îú‚îÄ‚îÄ replication/
‚îÇ   ‚îú‚îÄ‚îÄ server_1.json        # Log de replica√ß√£o do server_1
‚îÇ   ‚îú‚îÄ‚îÄ server_2.json        # Log de replica√ß√£o do server_2
‚îÇ   ‚îú‚îÄ‚îÄ server_3.json        # Log de replica√ß√£o do server_3
‚îÇ   ‚îú‚îÄ‚îÄ berkeley_sync_server_1.json  # Hist√≥rico de sincroniza√ß√£o Berkeley
‚îÇ   ‚îú‚îÄ‚îÄ berkeley_sync_server_2.json
‚îÇ   ‚îî‚îÄ‚îÄ berkeley_sync_server_3.json
```

**Exemplo `replication/server_1.json`:**
```json
{
  "server": "server_1",
  "log": [
    {
      "timestamp": 1698765440.789,
      "source": "server_2",
      "type": "messages",
      "records": 15
    },
    {
      "timestamp": 1698765450.123,
      "source": "server_3",
      "type": "channels",
      "records": 3
    }
  ]
}
```

---

### 8. Resumo: Garantias Fornecidas

| Propriedade | Status | Explica√ß√£o |
|-------------|--------|------------|
| **Disponibilidade** | ‚úÖ Alta | Sistema funciona com 1 de 3 servidores online |
| **Consist√™ncia** | ‚ö†Ô∏è Eventual | Janela de inconsist√™ncia de at√© 10 mensagens (~ 1-5s) |
| **Toler√¢ncia a Parti√ß√µes (CAP)** | ‚ö†Ô∏è Limitada | Prioriza A e P; consist√™ncia √© eventual |
| **Durabilidade** | ‚úÖ Alta | Dados persistidos em disco em cada servidor |
| **Ordena√ß√£o Causal** | ‚úÖ Garantida | Rel√≥gios l√≥gicos de Lamport |
| **Idempot√™ncia** | ‚úÖ Garantida | Replica√ß√£o por substitui√ß√£o completa |

**Classifica√ß√£o CAP:** **AP** (Availability + Partition Tolerance) com consist√™ncia eventual

---

## üöÄ Como Executar

### Pr√©-requisitos

- **Docker** (>= 20.10)
- **Docker Compose** (>= 2.0)
- **Git**

### Passo a Passo

#### 1. Clone o Reposit√≥rio

```bash
git clone <url-do-repositorio>
cd bbs
```

#### 2. Construa as Imagens Docker

```bash
cd docker
docker compose build
```

Este comando ir√°:
- Compilar o broker em C
- Instalar depend√™ncias Node.js para proxy e cliente
- Instalar depend√™ncias Python para servidores e bots
- Criar imagens otimizadas para cada componente

**Tempo estimado:** 3-5 minutos na primeira execu√ß√£o

#### 3. Inicie o Sistema

```bash
docker compose up
```

Ou, para rodar em segundo plano:

```bash
docker compose up -d
```

#### 4. Acompanhe os Logs

```bash
# Todos os containers
docker compose logs -f

# Container espec√≠fico
docker compose logs -f broker
docker compose logs -f server_1
docker compose logs -f bot_1
```

#### 5. Acesse o Cliente Interativo

```bash
docker attach bbs_client
```

Voc√™ ver√°:
```
=== Cliente BBS ===

Digite seu nome de usu√°rio: _
```

#### 6. Pare o Sistema

```bash
docker compose down
```

Para remover tamb√©m os volumes (dados persistidos):

```bash
docker compose down -v
```

---

## üß™ Testes Completos

Esta se√ß√£o cont√©m 13 testes para validar **todas** as funcionalidades do sistema, incluindo as novas features de replica√ß√£o, sincroniza√ß√£o Berkeley e hist√≥rico de mensagens.

### Teste 1: Login e Listagem de Usu√°rios

1. Inicie o sistema: `docker compose up`
2. Acesse o cliente: `docker attach bbs_client`
3. Fa√ßa login: `alice`
4. Liste usu√°rios: `/users`

**Resultado esperado:**
```
=== Usu√°rios Cadastrados ===
  - alice
  - bot_1
  - bot_2
============================
```

### Teste 2: Criar e Listar Canais

```
/create meucanal
/channels
```

**Resultado esperado:**
```
[CLIENT] Canal #meucanal criado com sucesso

=== Canais Dispon√≠veis ===
  - #geral
  - #tecnologia
  - #random
  - #meucanal
==========================
```

### Teste 3: Inscrever-se e Receber Mensagens de Canal

```
/join geral
```

Aguarde alguns segundos e observe mensagens dos bots:

```
[#geral] bot_1: Ol√° a todos! üëã
[#geral] bot_2: Como est√£o as coisas por aqui?
```

### Teste 4: Publicar Mensagem em Canal

```
/pub geral Ol√° pessoal!
```

### Teste 5: Mensagem Privada

1. No cliente 1: Login como `alice`
2. Abra outro terminal
3. No terminal 2: `docker exec -it bbs_client node client.js`
4. Login como `bob`
5. No cliente de alice: `/msg bob Oi Bob, tudo bem?`
6. No cliente de bob, voc√™ ver√°:

```
[@alice ‚Üí voc√™]: Oi Bob, tudo bem?
```

### Teste 6: Verificar Persist√™ncia

1. Publique algumas mensagens
2. Pare o sistema: `docker compose down`
3. Inicie novamente: `docker compose up`
4. Acesse o cliente e liste usu√°rios/canais

**Resultado esperado:** Dados anteriores foram recuperados

### Teste 7: Verificar Hist√≥rico de Mensagens

```
/history geral 20
```

**Resultado esperado:**
```
==================== Hist√≥rico de #geral (√∫ltimas 20 mensagens) ====================
[2024-01-15 14:30:21] alice: Ol√° pessoal!
[2024-01-15 14:30:35] bot_1: Ol√° a todos! üëã
[2024-01-15 14:30:42] bob: Oi Alice!
[2024-01-15 14:30:58] bot_2: Como est√£o as coisas?
...
====================================================================================
```

**Comandos dispon√≠veis:**
- `/history <canal>` - √öltimas 50 mensagens do canal
- `/history <canal> <limite>` - √öltimas N mensagens do canal

### Teste 8: Verificar Replica√ß√£o Entre Servidores

**Objetivo:** Verificar que todos os servidores eventualmente t√™m os mesmos dados

1. Envie 15+ mensagens para for√ßar sincroniza√ß√£o (acontece a cada 10 mensagens)
2. Verifique logs de replica√ß√£o:
```bash
docker compose logs server_1 | grep -i "Replicando estado"
docker compose logs server_2 | grep -i "Dados recebidos de"
docker compose logs server_3 | grep -i "Dados recebidos de"
```

**Resultado esperado:**
```
[SERVER:server_1] Replicando estado para todos os servidores
[SERVER:server_2] Dados recebidos de server_1 para replica√ß√£o
[SERVER:server_3] Dados recebidos de server_1 para replica√ß√£o
```

3. Verifique arquivos JSON em cada servidor:
```bash
docker exec bbs_server_1 cat /data/messages.json | jq 'length'
docker exec bbs_server_2 cat /data/messages.json | jq 'length'
docker exec bbs_server_3 cat /data/messages.json | jq 'length'
```

**Resultado esperado:** Mesmo n√∫mero de mensagens em todos os servidores

4. Verifique que comunica√ß√£o servidor-a-servidor (porta 6000) est√° funcionando:
```bash
docker compose logs | grep "6000\|replicate"
```

### Teste 9: Verificar Sincroniza√ß√£o Berkeley

**Objetivo:** Confirmar que rel√≥gios f√≠sicos est√£o sendo sincronizados

1. Envie 12+ mensagens para acionar sincroniza√ß√£o
2. Verifique logs do coordenador (rank=1, normalmente server_1):
```bash
docker compose logs server_1 | grep -i "berkeley\|offset\|Coletando timestamps"
```

**Resultado esperado:**
```
[SERVER:server_1] 10 mensagens processadas, acionando sincroniza√ß√£o
[SERVER:server_1] Executando sincroniza√ß√£o Berkeley
[SERVER:server_1] Coletando timestamps de 2 servidores
[SERVER:server_1] Calculando offsets. Tempo m√©dio: 1705330145.234
[SERVER:server_1] Offset para server_2: +0.012s
[SERVER:server_1] Offset para server_3: -0.008s
[SERVER:server_1] Distribuindo offsets para 2 servidores
```

3. Verifique logs dos servidores subordinados:
```bash
docker compose logs server_2 | grep -i "ajuste\|offset"
docker compose logs server_3 | grep -i "ajuste\|offset"
```

**Resultado esperado:**
```
[SERVER:server_2] Recebido ajuste de rel√≥gio: +0.012s
[SERVER:server_3] Recebido ajuste de rel√≥gio: -0.008s
```

### Teste 10: Teste de Falha e Recupera√ß√£o de Servidor

**Objetivo:** Verificar toler√¢ncia a falhas e recupera√ß√£o autom√°tica

1. Pare um servidor (n√£o o coordenador):
```bash
docker stop bbs_server_2
```

2. Continue usando o cliente normalmente - envie 5+ mensagens

**Resultado esperado:** Sistema continua funcionando, broker distribui para servidores restantes (server_1 e server_3)

3. Verifique que coordenador detecta falha:
```bash
docker compose logs server_1 | grep -i "server_2"
```

4. Reinicie o servidor:
```bash
docker start bbs_server_2
```

5. Verifique que servidor recupera dados via replica√ß√£o:
```bash
docker compose logs server_2 | grep -i "Sincronizando do coordenador"
```

**Resultado esperado:** Servidor se ressincroniza automaticamente e recebe dados perdidos

6. Verifique consist√™ncia de dados ap√≥s recupera√ß√£o:
```bash
docker exec bbs_server_2 cat /data/messages.json | jq 'length'
```

### Teste 10.1: Elei√ß√£o de Coordenador (Protocolo Bully)

**Objetivo:** Verificar que o algoritmo Bully funciona corretamente ao detectar falha do coordenador

**Pr√©-requisito:** Sistema rodando com 3 servidores

1. Identifique qual servidor √© o coordenador (normalmente server_1 com rank=1):
```bash
docker compose logs | grep "COORDENADOR"
```

2. Pare o coordenador para simular falha:
```bash
docker stop bbs_server_1
```

3. Aguarde 15-20 segundos e verifique logs dos servidores restantes:
```bash
docker compose logs server_2 | grep -i "elei√ß√£o\|election\|coordenador"
docker compose logs server_3 | grep -i "elei√ß√£o\|election\|coordenador"
```

**Resultado esperado:**
```
[SERVER:server_2] Coordenador server_1 n√£o responde. Iniciando elei√ß√£o.
[ELECTION:server_2] Iniciando elei√ß√£o (rank 2)
[ELECTION:server_2] Enviando requisi√ß√£o ELECTION para server_3 (rank 3)
[ELECTION:server_2] Recebido OK de server_3. Cancelando elei√ß√£o.
[SERVER:server_3] Requisi√ß√£o de elei√ß√£o recebida de server_2 (rank 2)
[ELECTION:server_3] Meu rank (3) √© maior. Respondendo OK e iniciando elei√ß√£o.
[ELECTION:server_3] Nenhum servidor com rank maior. Tornando-me coordenador.
[ELECTION:server_3] Sou o novo COORDENADOR (rank 3)
[ELECTION:server_3] Coordenador anunciado no t√≥pico 'servers'
[SERVER:server_2] An√∫ncio recebido: server_3 √© o novo coordenador (rank 3)
```

4. Verifique publica√ß√£o no t√≥pico `servers`:
```bash
docker compose logs | grep "servers.*new_coordinator"
```

5. Verifique log de elei√ß√£o persistido:
```bash
docker exec bbs_server_3 cat /data/replication/election_log.json
```

**Resultado esperado:** JSON com hist√≥rico de eventos de elei√ß√£o

6. Reinicie o coordenador original:
```bash
docker start bbs_server_1
```

7. Verifique que ele N√ÉO retoma coordena√ß√£o (server_3 continua como coordenador):
```bash
docker compose logs server_1 | grep -i "coordenador"
```

**Resultado esperado:**
```
[SERVER:server_1] An√∫ncio recebido: server_3 √© o novo coordenador (rank 3)
[SERVER:server_1] Coordenador atualizado: None -> server_3
```

**Observa√ß√µes:**
- Servidor com **maior rank** vence a elei√ß√£o (Bully usa rank como crit√©rio)
- Elei√ß√£o ocorre em < 20 segundos
- Todos os servidores s√£o notificados via t√≥pico `servers`
- Log de elei√ß√£o √© persistido em `/data/replication/election_log.json`

### Teste 11: Rel√≥gio L√≥gico (Lamport)

**Objetivo:** Verificar ordena√ß√£o causal de eventos

Observe nos logs o valor do `clock` aumentando:

```bash
docker compose logs -f server_1 | grep "clock"
```

**Resultado esperado:**
```
[SERVER:server_1] clock: 42
[SERVER:server_1] clock: 43
[SERVER:server_1] clock: 45  # Pode pular devido a mensagens de outros processos
```

**Interpreta√ß√£o:**
- Cada mensagem incrementa o rel√≥gio l√≥gico
- Valores maiores indicam eventos mais recentes (ordena√ß√£o causal)
- Saltos indicam sincroniza√ß√£o com rel√≥gios de outros processos

### Teste 12: Heartbeat do Reference Server

**Objetivo:** Verificar que todos os servidores est√£o registrando presen√ßa

```bash
docker compose logs -f reference | grep "heartbeat"
```

**Resultado esperado:**
```
[REFERENCE] Servidor server_1 enviou heartbeat
[REFERENCE] Servidor server_2 enviou heartbeat
[REFERENCE] Servidor server_3 enviou heartbeat
```

**Frequ√™ncia esperada:** A cada 5 segundos por servidor

### Teste 13: Verificar Bots Autom√°ticos

```bash
docker compose logs bot_1 | tail -50
```

**Resultado esperado:**
```
[BOT:bot_1] Enviando 10 mensagens para #geral
[BOT:bot_1] Mensagem 1/10 enviada para #geral (total: 1, clock: 15)
[BOT:bot_1] Mensagem 2/10 enviada para #geral (total: 2, clock: 17)
...
[BOT:bot_1] Aguardando antes de escolher novo canal...
```

---

## üìÅ Estrutura de Diret√≥rios

```
bbs/
‚îú‚îÄ‚îÄ c/                          # C√≥digo em C
‚îÇ   ‚îú‚îÄ‚îÄ broker/                 # Broker ROUTER-DEALER
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ broker.c
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Makefile
‚îÇ   ‚îú‚îÄ‚îÄ common_utils/           # Utilit√°rios comuns
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logical_clock.h
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logical_clock.c
‚îÇ   ‚îî‚îÄ‚îÄ server/                 # (Opcional) Servidor em C
‚îÇ
‚îú‚îÄ‚îÄ javascript/                 # C√≥digo em JavaScript/Node.js
‚îÇ   ‚îú‚îÄ‚îÄ client/                 # Cliente interativo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ proxy/                  # Proxy XSUB-XPUB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ proxy.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ common_utils/           # Utilit√°rios comuns
‚îÇ       ‚îú‚îÄ‚îÄ logicalClock.js
‚îÇ       ‚îú‚îÄ‚îÄ messaging.js
‚îÇ       ‚îî‚îÄ‚îÄ index.js
‚îÇ
‚îú‚îÄ‚îÄ python/                     # C√≥digo em Python
‚îÇ   ‚îú‚îÄ‚îÄ bot/                    # Bot autom√°tico
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bot.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ server/                 # Servidor de mensagens
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ reference_server/       # Servidor de refer√™ncia
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reference_server.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ common_utils/           # Utilit√°rios comuns
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ logical_clock.py
‚îÇ       ‚îú‚îÄ‚îÄ persistence.py
‚îÇ       ‚îî‚îÄ‚îÄ messaging.py
‚îÇ
‚îú‚îÄ‚îÄ data/                       # Persist√™ncia local
‚îÇ   ‚îú‚îÄ‚îÄ logins.json
‚îÇ   ‚îú‚îÄ‚îÄ channels.json
‚îÇ   ‚îú‚îÄ‚îÄ messages.json
‚îÇ   ‚îú‚îÄ‚îÄ reference.json
‚îÇ   ‚îî‚îÄ‚îÄ replication/
‚îÇ
‚îú‚îÄ‚îÄ docker/                     # Configura√ß√£o Docker
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.broker
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.proxy
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.reference
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.server
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.client
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.bot
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îÇ
‚îî‚îÄ‚îÄ README.md                   # Este arquivo
```

---

## üêõ Troubleshooting

### Problema: Containers n√£o iniciam

**Sintoma:**
```
ERROR: Service 'broker' failed to build
```

**Solu√ß√£o:**
1. Verifique se Docker est√° rodando: `docker ps`
2. Limpe containers antigos: `docker compose down -v`
3. Reconstrua: `docker compose build --no-cache`

### Problema: Cliente n√£o conecta ao broker

**Sintoma:**
```
[CLIENT] Erro ao conectar: ECONNREFUSED
```

**Solu√ß√£o:**
1. Verifique se broker est√° rodando: `docker compose ps`
2. Verifique logs do broker: `docker compose logs broker`
3. Aguarde alguns segundos (containers podem levar tempo para iniciar)
4. Reinicie o cliente: `docker compose restart client`

### Problema: Mensagens n√£o aparecem no cliente

**Sintoma:**
Cliente n√£o recebe publica√ß√µes de canais

**Solu√ß√£o:**
1. Verifique se inscreveu no canal: `/join nomecanal`
2. Verifique se proxy est√° rodando: `docker compose ps proxy`
3. Verifique logs do proxy: `docker compose logs proxy`

### Problema: Servidor n√£o sincroniza

**Sintoma:**
Servidores t√™m dados diferentes ap√≥s muitas mensagens

**Solu√ß√£o:**
1. Verifique se os 3 servidores est√£o rodando: `docker compose ps`
2. Verifique heartbeat no reference server: `docker compose logs reference`
3. Aguarde sincroniza√ß√£o (acontece a cada 10 mensagens)
4. Verifique logs de replica√ß√£o:
   ```bash
   docker compose logs server_1 | grep -i "replic"
   ```
5. Verifique conectividade servidor-a-servidor (porta 6000):
   ```bash
   docker compose exec server_1 nc -zv server_2 6000
   ```
6. Verifique sincroniza√ß√£o Berkeley:
   ```bash
   docker compose logs server_1 | grep -i "berkeley"
   ```

### Problema: Coordenador n√£o executa sincroniza√ß√£o Berkeley

**Sintoma:**
Logs n√£o mostram "Executando sincroniza√ß√£o Berkeley"

**Solu√ß√£o:**
1. Verifique qual servidor √© o coordenador (rank=1):
   ```bash
   docker compose logs | grep "Rank atribu√≠do"
   ```
2. Verifique se coordenador est√° processando mensagens:
   ```bash
   docker compose logs server_1 | grep "mensagens processadas"
   ```
3. Sincroniza√ß√£o s√≥ ocorre a cada 10 mensagens - envie mais mensagens
4. Verifique se outros servidores est√£o acess√≠veis:
   ```bash
   docker compose logs server_1 | grep "Coletando timestamps"
   ```

### Problema: Rel√≥gio l√≥gico n√£o incrementa

**Sintoma:**
Valor de `clock` n√£o aumenta nos logs

**Solu√ß√£o:**
1. Isso √© esperado se n√£o h√° mensagens sendo trocadas
2. Envie mensagens via cliente ou espere bots enviarem
3. Verifique implementa√ß√£o do rel√≥gio l√≥gico em cada componente

### Problema: Bot n√£o publica mensagens

**Sintoma:**
```
[BOT:bot_1] Falha ao enviar mensagem
```

**Solu√ß√£o:**
1. Verifique se servidores est√£o rodando
2. Verifique se broker est√° rodando
3. Reinicie o bot: `docker compose restart bot_1`
4. Verifique logs completos: `docker compose logs bot_1`

### Problema: Dados n√£o persistem

**Sintoma:**
Ap√≥s reiniciar, dados anteriores s√£o perdidos

**Solu√ß√£o:**
1. Verifique se volumes Docker est√£o criados: `docker volume ls | grep bbs`
2. N√£o use `docker compose down -v` (remove volumes)
3. Verifique permiss√µes do diret√≥rio `/data/`
4. Verifique logs de persist√™ncia: `docker compose logs server_1 | grep "salv"`

### Problema: "Permission denied" ao acessar cliente

**Sintoma:**
```
docker attach bbs_client
Error: permission denied
```

**Solu√ß√£o:**
1. Use `sudo docker attach bbs_client`
2. Ou adicione seu usu√°rio ao grupo docker:
```bash
sudo usermod -aG docker $USER
newgrp docker
```

### Problema: Portas j√° em uso

**Sintoma:**
```
ERROR: for broker  Cannot start service broker: 
Bind for 0.0.0.0:5555 failed: port is already allocated
```

**Solu√ß√£o:**
1. Verifique processos usando a porta: `sudo lsof -i :5555`
2. Mate o processo: `kill -9 <PID>`
3. Ou mude a porta no `docker-compose.yml`:
```yaml
ports:
  - "5565:5555"  # Mapeia porta externa 5565 para interna 5555
```

---

## üìö Refer√™ncias

- **ZeroMQ Guide:** https://zguide.zeromq.org/
- **MessagePack:** https://msgpack.org/
- **Lamport Clocks:** "Time, Clocks, and the Ordering of Events in a Distributed System" - Leslie Lamport, 1978
- **Berkeley Algorithm:** "Time Synchronization in Distributed Systems" - Tanenbaum & Van Steen
- **Docker Compose:** https://docs.docker.com/compose/

---

## üë• Autor

Desenvolvido como projeto da disciplina de **Sistemas Distribu√≠dos**.

**Licen√ßa:** MIT

---

## üìù Notas Finais

Este projeto implementa conceitos fundamentais de sistemas distribu√≠dos de forma pr√°tica e funcional. Embora seja uma implementa√ß√£o educacional, demonstra:

‚úÖ Comunica√ß√£o ass√≠ncrona eficiente
‚úÖ Balanceamento de carga autom√°tico
‚úÖ Sincroniza√ß√£o de rel√≥gios distribu√≠dos
‚úÖ Replica√ß√£o de dados com consist√™ncia
‚úÖ Persist√™ncia local robusta
‚úÖ Integra√ß√£o multi-linguagem
‚úÖ Containeriza√ß√£o e orquestra√ß√£o

**Poss√≠veis Melhorias Futuras:**
- Implementa√ß√£o completa do algoritmo de Berkeley
- Elei√ß√£o de coordenador mais robusta (Bully Algorithm)
- Criptografia de mensagens
- Autentica√ß√£o de usu√°rios
- Interface web para o cliente
- M√©tricas e monitoramento (Prometheus/Grafana)
- Testes automatizados (unit + integration)
- CI/CD pipeline

---

**üéâ Obrigado por usar o Sistema BBS!**

Para d√∫vidas ou contribui√ß√µes, abra uma issue no reposit√≥rio.
